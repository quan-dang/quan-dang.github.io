<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kinh nghiệm xương máu của bản thân trong quá trình triển khai các hệ thống Machine Learning</title>
      <link href="2021/04/26/System-Overview/"/>
      <url>2021/04/26/System-Overview/</url>
      
        <content type="html"><![CDATA[<div style="text-align: center"> Có nhiều cách triển khai một hệ thống Machine Learning. Có những hệ thống chỉ đơn thuần là một Web API, nhận vào một record ID, và trả về kết quả tương ứng đã được dự đoán từ trước (pre-compute). Có những hệ thống phức tạp hơn, bao gồm cả các thành phần để theo dõi model, theo dõi hệ thống, đánh giá data đầu vào, vân vân và mây mây v.v... Hôm nay mình xin chia sẻ một số kiến trúc cơ bản mà mình thường lấy làm cơ sở để phát triển các hệ thống Machine Learning. Tài liệu này được đúc rút ra từ chính kinh nghiệm và quá trình học hỏi của bản thân, do đó chắc chắn không khỏi mang tính chủ quan, mong mọi người thông cảm và tích cực đưa ra ý kiến.</div><p><img src="https://miro.medium.com/max/1000/1*SoA2-16rMISDOF3OGNjWiA.jpeg" alt="ML meme"></p>]]></content>
      
      
      
        <tags>
            
            <tag> system design </tag>
            
            <tag> ML System </tag>
            
            <tag> AI System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge Distillation với PyTorch</title>
      <link href="2021/01/24/Knowledge-Distillation-voi-PyTorch/"/>
      <url>2021/01/24/Knowledge-Distillation-voi-PyTorch/</url>
      
        <content type="html"><![CDATA[<div style="text-align: center"> Knowledge Distillation (KD) là một kỹ thuật nén model (model compression) sao cho độ chính xác (hoặc một thước đo khác như mAP, và F1-score, ...) không thay đổi nhiều so với model gốc. </div><p><img src="/2021/01/24/Knowledge-Distillation-voi-PyTorch/teach-me.jpg" alt="Random Forests meme"></p><h3 id="Bai-toan-thuc-te"><a href="#Bai-toan-thuc-te" class="headerlink" title="Bài toán thực tế"></a>Bài toán thực tế</h3><p>Nén model phân loại chữ viết tay sử dụng kỹ thuật KD, với bộ dữ liệu <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></p><h4 id="Buoc-1-Import-cac-thu-vien-can-thiet"><a href="#Buoc-1-Import-cac-thu-vien-can-thiet" class="headerlink" title="Bước 1: Import các thư viện cần thiết"></a>Bước 1: Import các thư viện cần thiết</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> vstack</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><h4 id="Buoc-2-Load-du-lieu"><a href="#Buoc-2-Load-du-lieu" class="headerlink" title="Bước 2: Load dữ liệu"></a>Bước 2: Load dữ liệu</h4><p>Ở bước này chúng ta sẽ load bộ dữ liệu MNIST,và biến đổi dữ liệu sao cho model có thể sử dụng được. Lưu ý rằng bộ dữ liệu này có sẵn trong thư viện torchvision rồi, nên chỉ cần load lên mà không cần vào trang chủ của bộ dữ liệu để tải nha. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># định nghĩa phép chuyển đổi: thay đổi kích thước ảnh thành 32x32, và chuyển đổi</span></span><br><span class="line"><span class="comment"># dữ liệu sang dạng tensor</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">                                transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                ])</span><br><span class="line"><span class="comment"># load tập dữ liệu train và test, và áp dụng phép chuyển đổi cho tập dữ liệu</span></span><br><span class="line">train_set = MNIST(root=<span class="string">&#x27;tmp/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_set = MNIST(root=<span class="string">&#x27;tmp/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tạo loader để truyền vào model dữ liệu theo batch</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="Buoc-3-Xay-dung-cac-model"><a href="#Buoc-3-Xay-dung-cac-model" class="headerlink" title="Bước 3: Xây dựng các model"></a>Bước 3: Xây dựng các model</h4><p>Chúng ta sẽ xây dựng 2 model ở bước này:</p><ul><li>Model giáo viên (teacher): model gốc (LeNet5)</li><li>Model học sinh (student): model sau khi nén bằng cách giữ nguyên số lượng layer, nhưng giảm số lượng tham số (parameters)</li></ul><h5 id="Model-giao-vien"><a href="#Model-giao-vien" class="headerlink" title="Model giáo viên"></a>Model giáo viên</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Teacher</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Ref: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</span></span><br><span class="line">        <span class="built_in">super</span>(Teacher, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><h5 id="Model-hoc-sinh"><a href="#Model-hoc-sinh" class="headerlink" title="Model học sinh"></a>Model học sinh</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Student, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">12</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">12</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">90</span>)  <span class="comment"># 6*6 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">90</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><h4 id="Buoc-3-Train-cac-model"><a href="#Buoc-3-Train-cac-model" class="headerlink" title="Bước 3: Train các model"></a>Bước 3: Train các model</h4><h5 id="Model-giao-vien-1"><a href="#Model-giao-vien-1" class="headerlink" title="Model giáo viên"></a>Model giáo viên</h5><p>Đầu tiên chúng ta sẽ định nghĩa hàm loss và optimizer cho model như sau:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(teacher.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>Sau đó train model 100 epochs;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = teacher(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># print every 100 mini-batches</span></span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">100</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><h5 id="Model-hoc-sinh-1"><a href="#Model-hoc-sinh-1" class="headerlink" title="Model học sinh"></a>Model học sinh</h5><p>Ngoài định nghĩa hàm loss và optimizer như model giáo viên, chúng ta sẽ khai báo thêm 2 tham số cố định $alpha$ và $temperature$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">temperature = <span class="number">10</span></span><br></pre></td></tr></table></figure><p>Và train model 50 epochs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">          teacher_pred = teacher(inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        student_pred = student(inputs)</span><br><span class="line"></span><br><span class="line">        student_loss = criterion(student_pred, labels)</span><br><span class="line">        </span><br><span class="line">        distillation_loss = F.kl_div(</span><br><span class="line">            F.log_softmax(teacher_pred / temperature, dim=<span class="number">1</span>),</span><br><span class="line">            F.softmax(student_pred / temperature, dim=<span class="number">1</span>),</span><br><span class="line">            reduction=<span class="string">&#x27;batchmean&#x27;</span>        </span><br><span class="line">        )</span><br><span class="line">        loss = alpha * student_loss + (<span class="number">1</span> - alpha) * distillation_loss</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># print every 100 mini-batches</span></span><br><span class="line">            print(<span class="string">&#x27;[&#123;&#125;, &#123;&#125;] loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">100</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>Lưu ý một vài thay đổi so với model giáo viên:</strong> </p><table><thead><tr><th align="center"><img src="/2021/01/24/Knowledge-Distillation-voi-PyTorch/knowledge_distillation.png" alt="Knowledge Distillation"></th></tr></thead><tbody><tr><td align="center"><em>Nguồn: <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a></em></td></tr></tbody></table><ul><li>model học sinh sử dụng thêm 1 loại loss nữa, đó là $distillation\ loss$, được tính bằng <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> giữa phân phối của model giáo viên và model học sinh.</li><li>Tham số $alpha$ dùng để đánh trọng số (weight) cho $student\ loss$ và $distillation\ loss$.</li><li>Tham số $temperature$ có tác dụng làm phân phối “soft” hơn. Tham số này càng tăng, thì phân phối càng “soft”, như hình dưới đây:</li></ul><p><img src="/2021/01/24/Knowledge-Distillation-voi-PyTorch/smoothen.png" alt="Smooth Distribution"></p><h3 id="Tai-sao-khong-train-model-hoc-sinh-tu-dau"><a href="#Tai-sao-khong-train-model-hoc-sinh-tu-dau" class="headerlink" title="Tại sao không train model học sinh từ đầu?"></a>Tại sao không train model học sinh từ đầu?</h3><p>Chúng ta hoàn toàn có thể train model học sinh từ đầu, tuy nhiên sẽ khó đạt được kết quả như kỳ vọng. Kỹ thuật KD giúp model học sinh:</p><ul><li>Generalize tốt hơn, do tập train của model giáo viên thông thường sẽ lớn hơn nhiều so với tập train của model học sinh</li><li>$distillation\ loss$ giảm thiểu ảnh hưởng của việc phân bố tập train của model học sinh khác nhiều so với phân bố thực tế</li></ul><h3 id="Tai-lieu-tham-khao"><a href="#Tai-lieu-tham-khao" class="headerlink" title="Tài liệu tham khảo"></a>Tài liệu tham khảo</h3><p>[1] <a href="https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764">https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764</a><br>[2] <a href="https://keras.io/examples/vision/knowledge_distillation/">https://keras.io/examples/vision/knowledge_distillation/</a><br>[3] <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a><br>[4] <a href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a><br>[5] <a href="https://thenextweb.com/neural/2020/10/26/how-knowledge-distillation-compresses-neural-networks-syndication/">https://thenextweb.com/neural/2020/10/26/how-knowledge-distillation-compresses-neural-networks-syndication/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> knowledge distillation </tag>
            
            <tag> model compression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Trees</title>
      <link href="2021/01/18/Decision-Trees-DTs/"/>
      <url>2021/01/18/Decision-Trees-DTs/</url>
      
        <content type="html"><![CDATA[<div style="text-align: center"> Decision Trees là một trong những thuật toán phổ biến trong Machine Learning, là tiền đề để phát triển các thuật toán phức tạp hơn như Random Forest, XGBoost và LightGBM. Hôm nay mình sẽ cùng nhau tìm hiểu thuật toán này nhé. </div><p><img src="https://hackernoon.com/hn-images/1*6-ctYxKmTS8v0RzwLILfXA.png" alt="Random Forests meme"></p><h3 id="Bai-toan-thuc-te"><a href="#Bai-toan-thuc-te" class="headerlink" title="Bài toán thực tế"></a>Bài toán thực tế</h3><h4 id="Phan-loai-hoa-voi-tap-du-lieu-iris"><a href="#Phan-loai-hoa-voi-tap-du-lieu-iris" class="headerlink" title="Phân loại hoa với tập dữ liệu iris"></a>Phân loại hoa với <a href="https://archive.ics.uci.edu/ml/datasets/iris">tập dữ liệu iris</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># load dữ liệu iris</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># tách features (X) và label (y)</span></span><br><span class="line">X = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># chia dữ liệu thành 2 tập train và test, với tỷ lệ </span></span><br><span class="line"><span class="comment"># tập test là 0.2</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xây dựng model</span></span><br><span class="line">clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># train model</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dự đoán trên tập test</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># đánh giá kết quả dựa trên accuracy score</span></span><br><span class="line">print(<span class="string">&quot;[INFO] accuracy score: &quot;</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><h4 id="Du-doan-gia-nha-voi-tap-du-lieu-boston"><a href="#Du-doan-gia-nha-voi-tap-du-lieu-boston" class="headerlink" title="Dự đoán giá nhà với tập dữ liêu boston"></a>Dự đoán giá nhà với <a href="https://www.kaggle.com/vikrishnan/boston-house-prices">tập dữ liêu boston</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># load dữ liệu boston</span></span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># tách features (X) và label (y)</span></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># chia dữ liệu thành 2 tập train và test, với tỷ lệ </span></span><br><span class="line"><span class="comment"># tập test là 0.2</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xây dựng model</span></span><br><span class="line">clf = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># train model</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dự đoán trên tập test</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># đánh giá kết quả dựa trên r2_score</span></span><br><span class="line">print(<span class="string">&quot;[INFO] r2_score: &quot;</span>, r2_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><h3 id="Y-tuong-thuat-toan"><a href="#Y-tuong-thuat-toan" class="headerlink" title="Ý tưởng thuật toán"></a>Ý tưởng thuật toán</h3><p>Hiểu đơn giản là thuật toán xây dựng một cây quyết định, liên tục phân chia tập dữ liệu về các nhánh. Thuật toán này có thể dùng cho cả bài toán phân loại (classification) và hồi quy (regression).</p><p>Có nhiều phương pháp implement thuật toán Decision Trees, ví dụ như ID3, C4.5, C5.0 và CART. Thư viện sklearn sử dụng phiên bản tối ưu của CART.</p><h4 id="Phan-loai"><a href="#Phan-loai" class="headerlink" title="Phân loại"></a>Phân loại</h4><p>Trở lại bài toán phân loại hoa, sử dụng đoạn code dưới đây để xem cây quyết định trông thế nào nhé.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dot_data = export_graphviz(</span><br><span class="line">            clf,</span><br><span class="line">            out_file=<span class="literal">None</span>,</span><br><span class="line">            feature_names=iris.feature_names,</span><br><span class="line">            class_names=iris.target_names,</span><br><span class="line">            rounded=<span class="literal">True</span>,</span><br><span class="line">            filled=<span class="literal">True</span>,</span><br><span class="line">            special_characters=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line"><span class="comment"># lưu graph ra file graph.png</span></span><br><span class="line">graph.render(<span class="string">&quot;iris-tree&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line"><span class="comment"># hiển thị graph</span></span><br><span class="line">graph</span><br></pre></td></tr></table></figure><p>Và bing-go, cây sẽ như này nè:</p><p><img src="/2021/01/18/Decision-Trees-DTs/iris-tree.png" alt="iris tree"></p><p><strong>Giải thích cây quyết định:</strong> </p><ul><li><p>Bắt đầu từ node trên cùng (hay node root), với điều kiện $petal\ width \leq 0.8$, dữ liệu nào thỏa mãn điều kiện sẽ đi về nhánh trái, còn lại sẽ đi về nhánh phải. Node màu cam không có nhánh con nào nên được gọi là node lá, và dữ liệu ở node này sẽ được kết luận luôn là thuộc class setosa. Node màu xám (có nhánh con nên được gọi là node phân chia hay splitting node) tiếp tục phân chia tập dữ liệu thuộc node này về 2 node xanh và tím dựa trên điều kiện $petal\ length \leq 4.75$</p></li><li><p>Một số thông tin khác của node bao gồm:</p><ul><li><p>gini: mô tả “độ thuần khiết” (impurity) của node, dao động từ 0 đến 1. Trong đó 0 nghĩa là tất cả dữ liệu thuộc cùng 1 class, nên chúng ta luôn muốn <strong>gini</strong> nhỏ nhất có thể </p><p>  Công thức tính <strong>gini</strong> cho node $i$ như sau: $G_i = 1-\sum_{k=1}^{n}p_{i,k}^2$</p><p>  Trong đó: </p><ul><li><p>$p_{i,k}$ là tỉ lệ số phần tử của class k trên tổng số samples.</p><p>  Ví dụ: ở node màu tím, $G=1-(0/43)^2-(5/43)^2-(38/43)^2\approx0.206$</p></li></ul></li><li><p>samples: số lượng dữ liệu training thuộc node</p></li><li><p>value: số lượng dữ liệu của từng class trong node, ví dụ node tím có 0 setosa, 5 versicolor và 38 virginica </p></li></ul></li></ul><h4 id="Hoi-quy"><a href="#Hoi-quy" class="headerlink" title="Hồi quy"></a>Hồi quy</h4><p>Cây quyết định cho bài toán dự đoán giá nhà như sau:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dot_data = export_graphviz(</span><br><span class="line">            clf,</span><br><span class="line">            out_file=<span class="literal">None</span>,</span><br><span class="line">            feature_names=boston.feature_names,</span><br><span class="line">            rounded=<span class="literal">True</span>,</span><br><span class="line">            filled=<span class="literal">True</span>,</span><br><span class="line">            special_characters=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line"><span class="comment"># lưu graph ra file graph.png</span></span><br><span class="line">graph.render(<span class="string">&quot;boston-tree&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line"><span class="comment"># hiển thị graph</span></span><br><span class="line">graph</span><br></pre></td></tr></table></figure><p><img src="/2021/01/18/Decision-Trees-DTs/boston-tree.png" alt="boston tree"></p><p>Cây quyết định cho bài toán hồi quy khá giống với phân loại, ngoại trừ:</p><ul><li><p><strong>gini</strong> được thay bằng <strong>MSE</strong></p><p>  Công thức tính <strong>MSE</strong> cho node $i$ như sau: $MSE_i = \sum_{i\in node}(y_i - \bar{y})^2$</p><p>  Trong đó: </p><ul><li>$y_i$ là label của từng phần tử trong node</li><li>$\bar{y} = \frac{1}{tổng\ số \ phần\ tử\ của\ node}\sum_{i\in node}y_i$</li></ul></li><li><p>dự đoán value, thay vì class</p></li></ul><h3 id="Phuong-phap-CART"><a href="#Phuong-phap-CART" class="headerlink" title="Phương pháp CART"></a>Phương pháp CART</h3><ol><li><p>Bước 1: chia tập dữ liệu thành 2 tập con sử dụng feature $k$ và giá trị $t_k$, sao cho hàm cost dưới đây là nhỏ nhất<br> $J(k,t_k)=\frac{m_{trái}}{m}G_{trái}+\frac{m_{phải}}{m}G_{phải}$</p><p> Trong đó:</p><ul><li>$m_{trái}$ và $m_{phải}$ là số lượng dữ liệu của 2 node trái và phải (2 tập con)</li><li>$m$ là số lượng dữ liệu ở node hiện tại</li><li>$G_{trái}$ và $G_{phải}$ là <strong>gini</strong> của 2 node trái và phải</li></ul></li><li><p>Bước 2: tiếp tục phân chia các tập con sử dụng logic tương tự bước trên, cho tới khi thỏa mãn điều kiện dừng, ví dụ như:</p><ul><li>đạt độ sâu tối đa (quy định bởi hyperparameter <strong>max_depth</strong>)</li><li>không thể phân chia sao cho <strong>gini</strong> của các tập con nhỏ hơn tập gốc</li></ul></li></ol><h3 id="Tai-lieu-tham-khao"><a href="#Tai-lieu-tham-khao" class="headerlink" title="Tài liệu tham khảo"></a>Tài liệu tham khảo</h3><p>[1] Aurlien Gron. 2020. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd. ed.). O’Reilly Media, Inc.<br>[2] Brownlee, J. (2016) Machine Learning Mastery with Python. Machine Learning Mastery, EBook.<br>[3] <a href="https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052">https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> decision trees </tag>
            
            <tag> random forest </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
