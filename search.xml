<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Model Serving sử dụng BentoML</title>
      <link href="2021/10/30/model-serving-using-bentoml/"/>
      <url>2021/10/30/model-serving-using-bentoml/</url>
      
        <content type="html"><![CDATA[<div style="text-align: center"> Khi mà một model đã được train xong và đạt được kết quả như ý muốn, thì đây chính là lúc đem đi để phục vụ (serve) các khách hàng với dữ liệu thực tế. Giai đoạn này được gọi là model serving. </div><img src="model-serving-meme.jpeg" alt="Model serving meme" width="500" height="500" />]]></content>
      
      
      
        <tags>
            
            <tag> model serving </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kiểm thử hệ thống Machine Learning</title>
      <link href="2021/08/22/test-in-ml/"/>
      <url>2021/08/22/test-in-ml/</url>
      
        <content type="html"><![CDATA[<p>Xin chào mọi người, lại là mình đây, hôm nay chúng mình sẽ cùng nhau tìm hiểu xem để kiểm thử một hệ thống Machine Learning (ML) thì nên làm những gì nào!<br>Thêm cái meme trước khi bắt đầu bài viết như mọi khi phát đã.</p><img src="ml-testing-meme.png" alt="ML testing meme" width="500" height="500" /><h3 id="Kiem-thu-he-thong-ML-de-nhu-an-banh"><a href="#Kiem-thu-he-thong-ML-de-nhu-an-banh" class="headerlink" title="Kiểm thử hệ thống ML dễ như ăn bánh?"></a>Kiểm thử hệ thống ML dễ như ăn bánh?</h3><p>Khác với các hệ thống software truyển thống: một ông developer ngồi nghĩ ra các rule, và lập trình bằng Python, Java, hoặc… LOLCODE, thì ML model sẽ tự sinh ra các rule sử dụng dữ liệu được cung cấp. Điều này đương nhiên là tốt, vì không phải rule nào ông developer cũng nghĩ ra được, tuy nhiên nó cũng có mặt trái của nó: rule được sinh ra có thể thay đổi, theo hướng tốt, xấu, hoặc bị BUG rồi các ông ạ :). Điều này dẫn tới việc kiểm thử và debug một hệ thống ML không hề đơn giản.</p><img src="ml-testing.png" alt="ML testing" width="600" height="70" /><h3 id="Kiem-thu-he-thong-software-truyen-thong"><a href="#Kiem-thu-he-thong-software-truyen-thong" class="headerlink" title="Kiểm thử hệ thống software truyền thống"></a>Kiểm thử hệ thống software truyền thống</h3><p>Thông thường có 2 loại kiểm thử phần mềm:</p><ul><li>Functional Testing: kiểm tra xem hệ thống đã đảm bảo yêu cầu về chức năng chưa, ví dụ ấn tắt windows update mãi mà nó vẫn update thì là fail rồi :)</li><li>Non-functional Testing: kiểm tra xem hệ thống có đáp ứng được kỳ vọng của khách hàng không, ví dụ tất cả người trên thế giới cùng ấn nút tham gia group MLOps VN thì group không được sập chả hạn.</li></ul><h4 id="Functional-Testing"><a href="#Functional-Testing" class="headerlink" title="Functional Testing"></a>Functional Testing</h4><p>Thông thường loại này bao gồm:</p><ol><li><p><strong>Unit testing:</strong> test từng module nhỏ</p></li><li><p><strong>Integration testing:</strong> test một module lớn bao gồm nhiều module nhỏ để đảm bảo khi kết hợp không xảy ra vấn đề gì</p><p> <strong>Mẹo nhỏ:</strong> Theo nguyên tắc <a href="https://people.apache.org/~fhanik/kiss.html">KISS</a>, hãy luôn cố gắng bẻ vấn đề thành nhiều module đủ nhỏ và đủ dễ hiểu. </p><p> Ví dụ dưới đây được trích từ <a href="https://machinelearningmastery.com/machine-learning-in-python-step-by-step/">Machine learning mastery blog</a> cho thấy tác giả áp dụng rất tốt nguyên tắc này, qua việc tác giả đã cố gắng sử dụng hàm nhiều nhất có thể, ví dụ <code>train_test_split</code>, <code>accuracy_score</code> và <code>confusion_matrix</code>, khi đó chuyện test và debug sẽ dễ dàng hơn rất nhiều.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make predictions</span></span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="comment"># Load dataset</span></span><br><span class="line">url = <span class="string">&quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&quot;</span></span><br><span class="line">names = [<span class="string">&#x27;sepal-length&#x27;</span>, <span class="string">&#x27;sepal-width&#x27;</span>, <span class="string">&#x27;petal-length&#x27;</span>, <span class="string">&#x27;petal-width&#x27;</span>, <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">dataset = read_csv(url, names=names)</span><br><span class="line"><span class="comment"># Split-out validation dataset</span></span><br><span class="line">array = dataset.values</span><br><span class="line">X = array[:,<span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">y = array[:,<span class="number">4</span>]</span><br><span class="line">X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=<span class="number">0.20</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Make predictions on validation dataset</span></span><br><span class="line">model = SVC(gamma=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line">predictions = model.predict(X_validation)</span><br><span class="line"><span class="comment"># Evaluate predictions</span></span><br><span class="line">print(accuracy_score(Y_validation, predictions))</span><br><span class="line">print(confusion_matrix(Y_validation, predictions))</span><br><span class="line">print(classification_report(Y_validation, predictions))</span><br></pre></td></tr></table></figure></li><li><p><strong>Regression testing:</strong> kiểm tra lại toàn bộ chức năng của hệ thống mỗi khi có thay đổi của một hoặc vài chức năng nào đó</p></li><li><p><strong>Smoke testing:</strong> chạy một bài test cơ bản với chức năng tối thiểu để xem hệ thống sẵn sàng cho việc test chưa<br> Một ví dụ đơn giản: Bắt đầu kiểm tra một hệ thống bóng đèn, vừa ấn nút xong khói (smoke) bốc lên nghi ngút thì khỏi test tiếc gì thêm.</p></li></ol><h4 id="Non-functional-Testing"><a href="#Non-functional-Testing" class="headerlink" title="Non-functional Testing"></a>Non-functional Testing</h4><ol><li><strong>Load testing:</strong> xác định độ chịu tải, SLA của hệ thống</li><li><strong>Stress testing:</strong> đánh giá hành vi của hệ thống tại các điều kiện không lường trước, ví dụ một phần hệ thống đột nhiên shutdown thì phản hồi có chấp nhận được không</li></ol><h4 id="Luong-phat-trien-phan-mem-co-ban"><a href="#Luong-phat-trien-phan-mem-co-ban" class="headerlink" title="Luồng phát triển phần mềm cơ bản"></a>Luồng phát triển phần mềm cơ bản</h4><p>Thông thường, các developer tuân thủ một số quy ước sau khi phát triển phần mềm:</p><ol><li>Không merge code nếu chưa chạy các test case</li><li>Luôn viết code test khi commit logic mới</li><li>Khi fix bug, luôn viết code test để bắt bug, và phòng xảy ra trường hợp tương tự trong tương lai</li></ol><img src="typical_workflow_se.jpeg" alt="Typical SE development workflow" width="600" height="150" /><figcaption align="center" font-size="8px"><i>Source: https://www.jeremyjordan.me/testing-ml/</i></figcaption><h3 id="He-thong-ML-can-kiem-thu-nhung-gi"><a href="#He-thong-ML-can-kiem-thu-nhung-gi" class="headerlink" title="Hệ thống ML cần kiểm thử những gì?"></a>Hệ thống ML cần kiểm thử những gì?</h3><p>Những bài test cho hệ thống phần mềm có thể ứng dụng cho hầu hết ML code, tuy nhiên vẫn chưa để đủ đảm bảo hệ thống ML có thể hoạt động với độ tin cậy cao. </p><img src="difference.png" alt="ML testing differences" width="600" height="300" /><figcaption align="center" font-size="8px"><i>Source: https://learning.oreilly.com/library/view/building-machine-learning</i></figcaption><p>OK! Thế để hệ thống ML tin tưởng được thì cần kiểm tra thêm những gì?</p><ul><li><strong>Data pipeline testing:</strong> đảm bảo dữ liệu không bị corrupt, đúng format và đúng schema (kiểu dữ liệu), …</li><li><strong>Model testing:</strong> đảm bảo model đạt hiểu quả (ví dụ accuracy) như mong muốn và model có consistent không, …</li></ul><ol><li><strong>Data pipeline testing</strong><br>Data là một phần không thể thiếu trong một hệ thống ML, do đó duy trì một data pipeline với độ tin cậy cao là điều rất quan trọng.<br>Hình dưới đây là ví dụ về 1 data pipeline và những thứ yếu tố cần cân nhắc ở mỗi bước:</li></ol><img src="data_pipeline_testing.png" alt="Data pipeline testing" width="600" height="300" /><ol start="2"><li><strong>Model testing</strong><br>Có thể chia thành 2 loại model testing:</li></ol><ul><li><p><strong>Testing</strong></p><ul><li><p><strong>Pre-train testing:</strong> tìm bug trước khi train/evaluate</p><ul><li>Kiểm tra xem có data leakage (leak thông tin), ví dụ observation trong tập train cũng có ở tập validation/test</li><li>Kiểm tra xem có feature leakage (feature mang thông tin của label)</li><li>Kiểm tra model output có shape, hoặc có miền giá trị như ý muốn</li></ul></li><li><p><strong>Post-train testing:</strong> hoạt động của model (model behavior) có như ý muốn ở các tình huống (scenarios) khác nhau?</p><ul><li><strong>Invariance testing:</strong> mô tả những thay đổi của input mà không làm thay đổi kết quả dự đoán của model<br>  Ví dụ: trong bài toán sentiment analysis, thì 2 câu sau nên có cùng một output:<ul><li>Bộ phim A hay quá!</li><li>Bộ phim B hay quá!</li></ul></li><li><strong>Directional expectation test:</strong> mô tả những thay đổi của input sẽ làm thay đổi kết quả dự đoán của model một cách có thể lường trước.<br>  Ví dụ: trong bài toán dự đoán giá nhà, có thể đoán trước nếu không gian tăng, thì giá nhà sẽ tăng.</li><li><strong>Bias/Fairness:</strong> kiểm tra xem model có dự đoán công bằng không, ví dụ dự đoán income của người châu Mỹ chính xác hơn người châu Á, chứng tỏ model đang bị bias.</li><li><strong>Model Output Consistency:</strong> với cùng 1 dữ liệu đầu vào, model output có bị thay đổi sau nhiều lần chạy khác nhau không?</li></ul><p>  <strong>Note:</strong> Công cụ <a href="https://pair-code.github.io/what-if-tool/">What-If</a> hỗ trợ rất tốt trong việc kiểm tra model behavior ở các tình huống khác nhau.</p></li></ul></li><li><p><strong>Evaluation:</strong> đánh giá hiệu quả của model thông qua các metrics như accuracy và F1, … trên tập validation/test.</p></li></ul><h3 id="Mot-so-tool-hay-dung-de-test-va-debug"><a href="#Mot-so-tool-hay-dung-de-test-va-debug" class="headerlink" title="Một số tool hay dùng để test và debug"></a>Một số tool hay dùng để test và debug</h3><ul><li><a href="https://docs.python.org/3/library/pdb.html">pdb</a> để debug python code</li><li><a href="https://docs.pytest.org/en/6.2.x/">pytest</a> là framework hỗ trợ viết code test</li><li><a href="https://coverage.readthedocs.io/en/coverage-5.5/#:~:text=Coverage.py%20is%20a%20tool,gauge%20the%20effectiveness%20of%20tests.">Coverage.py</a> để xác định đoạn code nào có thể được execute nhưng đã không</li><li><a href="https://www.pylint.org/">pylint</a> để kiểm tra lỗi cú pháp/logic</li></ul><h3 id="Tai-lieu-tham-khao"><a href="#Tai-lieu-tham-khao" class="headerlink" title="Tài liệu tham khảo"></a>Tài liệu tham khảo</h3><p>[1] <a href="https://serokell.io/blog/machine-learning-testing">https://serokell.io/blog/machine-learning-testing</a><br>[2] <a href="https://developers.google.com/machine-learning/testing-debugging/common/overview">https://developers.google.com/machine-learning/testing-debugging/common/overview</a><br>[3] Emmanuel Ameisen, Building Machine Learning Powered Applications: Going from Idea to Product<br>[4] <a href="https://www.jeremyjordan.me/testing-ml/">https://www.jeremyjordan.me/testing-ml/</a><br>[5] <a href="https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf">https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf</a><br>[6] <a href="https://fontysblogt.nl/software-engineering-for-machine-learning-applications/">https://fontysblogt.nl/software-engineering-for-machine-learning-applications/</a><br>[7] <a href="https://futurice.com/blog/differences-between-machine-learning-and-software-engineering">https://futurice.com/blog/differences-between-machine-learning-and-software-engineering</a><br>[8] <a href="https://www.geeksforgeeks.org/differences-between-functional-and-non-functional-testing/">https://www.geeksforgeeks.org/differences-between-functional-and-non-functional-testing/</a><br>[9] <a href="https://eugeneyan.com/writing/testing-ml/">https://eugeneyan.com/writing/testing-ml/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> mlops </tag>
            
            <tag> ml-testing </tag>
            
            <tag> testing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tổng quan về hệ thống Machine Learning</title>
      <link href="2021/08/01/ml-system-overview/"/>
      <url>2021/08/01/ml-system-overview/</url>
      
        <content type="html"><![CDATA[<p>Hồi mới nhận nhiệm vụ triển khai hệ thống ML, mình cứ nghĩ là chắc build một Web API đơn giản là xong. Tuy nhiên khi thực sự bắt tay vào làm, cộng thêm đọc tài liệu trên trời dưới đất, mình mới chợt nhận ra là “Ôi thôi ngu rồi, API chỉ là một phần nhỏ xíu thôi”. Hy vọng bài viết này giúp mọi người có cái nhìn tổng quan hơn về một hệ thống ML, để khi nhận task đỡ bỡ ngỡ…</p><img src="sad-smile.jpeg" alt="ML system overview meme" width="300" height="200" /><p>Do tính tình bộc trực thẳng thắn nên mình xin phép đi thẳng vào vấn đề chính bằng cách ném toẹt bức ảnh tổng quan về hệ thống ML ở đây luôn.</p><img src="ml-sys.png" alt="ML system overview" width="630" height="500" /><p>Ở đây, ngoài môi trường để Data Scientist (DS) thử nghiệm model, mình dùng thêm 2 môi trường là staging và production.</p><h3 id="1-Moi-truong-staging"><a href="#1-Moi-truong-staging" class="headerlink" title="1. Môi trường staging"></a>1. Môi trường staging</h3><ul><li>Thực hiện unit test để đảm bảo tính chính xác của các hàm riêng lẻ</li><li>Thực hiện integration test để khi kết hợp các hàm với nhau không xảy ra vấn đề gì</li><li>Test training pipeline với dữ liệu staging (dữ liệu giả, hoặc lấy một phần dữ liêụ production) để xem có lỗi không<br>  <strong>Ví dụ:</strong> Lỗi pipeline không trả ra model, lỗi thiếu quyền truy cập vào các tài nguyên cloud (GCS,S3), …</li><li>Test luồng từ khi model được upload lên model registry cho tới khi serve model<br>  <strong>Ví dụ:</strong> Online serving có serve được request và trả ra kết quả đúng như dự đoán không</li></ul><h3 id="2-Moi-truong-production"><a href="#2-Moi-truong-production" class="headerlink" title="2. Môi trường production"></a>2. Môi trường production</h3><ul><li>Thực hiện train với dữ liệu production, và retrain tuỳ theo bài toán<br>  <strong>Ví dụ:</strong> Với dữ liệu training biến đổi nhiều thì retrain hàng ngày, còn nếu ít thì chỉ retrain khi nhận được event báo có sự thay đổi nhiều trong phân bố của training data </li><li>Thu thập ML metadata, ví dụ:<ul><li>Training pipeline metadata: ghi lại thời gian retrain, phân bố của dữ liệu training, kết quả model trên tập validation</li><li>Serving metadata: thông tin các model đang được serve hiện tại, hoặc model đang được A/B test</li></ul></li><li>Trả về kết quả dự đoán (có thể kèm giải thích cho dự đoán tuỳ model và bài toán) cho user, và lưu lại log để phân tích thêm<br>  Có 2 cách thức serve phổ biến<ul><li>Online serving: dự đoán mỗi khi có request đến</li><li>Offline batch serving: dự đoán trên toàn bộ dữ liệu, và mỗi khi có request, chỉ cần trả query kết quả dự đoán tương ứng</li></ul></li><li>Monitor hệ thống<br>  <strong>Ví dụ:</strong> monitor tình trạng CPU, GPU để scale tài nguyên lên hoặc xuống cho phù hợp, hoặc monitor thời gian phản hồi của model xem có bị chậm không</li><li>Monitor model: sử dụng log của bước dự đoán, kết hợp cùng dữ liệu có sẵn (ví dụ label cho user) để đánh giá xem model đã bị giảm performance chưa, hoặc có sự biến đổi nhiều trong phân bố của kết quả dự đoán không, vân vân và mây mây</li></ul><h3 id="3-Ket-luan"><a href="#3-Ket-luan" class="headerlink" title="3. Kết luận"></a>3. Kết luận</h3><ul><li> Hệ thống ML không chỉ đơn giản là làm xong một Web API rồi ném đấy. Mình cần xem xét các yếu tố khác, ví dụ như làm sao để quá trình từ lúc model còn đang ở môi trường research lên môi trường production nhanh nhất, hoặc khi nào thì cần thay thế một model đang trên production rồi, và thay thế như nào để giảm downtime, và nhiều thứ khác</li><li>Tuỳ từng yêu cầu bài toàn, mà hệ thống ML sẽ được phát triển theo các hướng khác nhau, với các thành phần khác nhau, do đó nên xác định rõ yêu cầu và timeline từ đầu, để có định hướng xây dựng hệ thống phù hợp</li></ul><h3 id="4-Tai-lieu-tham-khao"><a href="#4-Tai-lieu-tham-khao" class="headerlink" title="4. Tài liệu tham khảo"></a>4. Tài liệu tham khảo</h3><ol><li><a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</a></li><li><a href="https://github.com/mercari/ml-system-design-pattern">https://github.com/mercari/ml-system-design-pattern</a></li><li><a href="https://stanford-cs329s.github.io/">https://stanford-cs329s.github.io/</a></li><li><a href="https://docs.seldon.io/projects/seldon-core/en/latest/">https://docs.seldon.io/projects/seldon-core/en/latest/</a></li><li>Michael Munn, Sara Robinson, and Valliappa Lakshmanan: Machine Learning Design Patterns (2020)</li></ol><p>Bài viết đã dài phết rồi, mình xin được dừng bút tại đây. Mình sẽ cố gắng viết cụ thể hơn về từng thành phần trong các bài sau, mọi người nhớ đón đọc và thả tim nha. Cảm ơn mọi người và chúc mọi người giữ vững sức khoẻ trong mùa dịch :D.</p>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning system </tag>
            
            <tag> mlops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge Distillation với PyTorch</title>
      <link href="2021/01/24/knowledge-distillation-using-pytorch/"/>
      <url>2021/01/24/knowledge-distillation-using-pytorch/</url>
      
        <content type="html"><![CDATA[<div style="text-align: center"> Knowledge Distillation (KD) là một kỹ thuật nén model (model compression) sao cho độ chính xác (hoặc một thước đo khác như mAP, và F1-score, ...) không thay đổi nhiều so với model gốc. </div><p><img src="Knowledge-Distillation-voi-PyTorch/teach-me.jpg" alt="Random Forests meme"></p><h3 id="Bai-toan-thuc-te"><a href="#Bai-toan-thuc-te" class="headerlink" title="Bài toán thực tế"></a>Bài toán thực tế</h3><p>Nén model phân loại chữ viết tay sử dụng kỹ thuật KD, với bộ dữ liệu <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></p><h4 id="Buoc-1-Import-cac-thu-vien-can-thiet"><a href="#Buoc-1-Import-cac-thu-vien-can-thiet" class="headerlink" title="Bước 1: Import các thư viện cần thiết"></a>Bước 1: Import các thư viện cần thiết</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> vstack</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><h4 id="Buoc-2-Load-du-lieu"><a href="#Buoc-2-Load-du-lieu" class="headerlink" title="Bước 2: Load dữ liệu"></a>Bước 2: Load dữ liệu</h4><p>Ở bước này chúng ta sẽ load bộ dữ liệu MNIST,và biến đổi dữ liệu sao cho model có thể sử dụng được. Lưu ý rằng bộ dữ liệu này có sẵn trong thư viện torchvision rồi, nên chỉ cần load lên mà không cần vào trang chủ của bộ dữ liệu để tải nha. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># định nghĩa phép chuyển đổi: thay đổi kích thước ảnh thành 32x32, và chuyển đổi</span></span><br><span class="line"><span class="comment"># dữ liệu sang dạng tensor</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">                                transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                ])</span><br><span class="line"><span class="comment"># load tập dữ liệu train và test, và áp dụng phép chuyển đổi cho tập dữ liệu</span></span><br><span class="line">train_set = MNIST(root=<span class="string">&#x27;tmp/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_set = MNIST(root=<span class="string">&#x27;tmp/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tạo loader để truyền vào model dữ liệu theo batch</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="Buoc-3-Xay-dung-cac-model"><a href="#Buoc-3-Xay-dung-cac-model" class="headerlink" title="Bước 3: Xây dựng các model"></a>Bước 3: Xây dựng các model</h4><p>Chúng ta sẽ xây dựng 2 model ở bước này:</p><ul><li>Model giáo viên (teacher): model gốc (LeNet5)</li><li>Model học sinh (student): model sau khi nén bằng cách giữ nguyên số lượng layer, nhưng giảm số lượng tham số (parameters)</li></ul><h5 id="Model-giao-vien"><a href="#Model-giao-vien" class="headerlink" title="Model giáo viên"></a>Model giáo viên</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Teacher</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Ref: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</span></span><br><span class="line">        <span class="built_in">super</span>(Teacher, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><h5 id="Model-hoc-sinh"><a href="#Model-hoc-sinh" class="headerlink" title="Model học sinh"></a>Model học sinh</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Student, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">12</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">12</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">90</span>)  <span class="comment"># 6*6 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">90</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><h4 id="Buoc-3-Train-cac-model"><a href="#Buoc-3-Train-cac-model" class="headerlink" title="Bước 3: Train các model"></a>Bước 3: Train các model</h4><h5 id="Model-giao-vien-1"><a href="#Model-giao-vien-1" class="headerlink" title="Model giáo viên"></a>Model giáo viên</h5><p>Đầu tiên chúng ta sẽ định nghĩa hàm loss và optimizer cho model như sau:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(teacher.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>Sau đó train model 100 epochs;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = teacher(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># print every 100 mini-batches</span></span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">100</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><h5 id="Model-hoc-sinh-1"><a href="#Model-hoc-sinh-1" class="headerlink" title="Model học sinh"></a>Model học sinh</h5><p>Ngoài định nghĩa hàm loss và optimizer như model giáo viên, chúng ta sẽ khai báo thêm 2 tham số cố định $alpha$ và $temperature$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">temperature = <span class="number">10</span></span><br></pre></td></tr></table></figure><p>Và train model 50 epochs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">          teacher_pred = teacher(inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        student_pred = student(inputs)</span><br><span class="line"></span><br><span class="line">        student_loss = criterion(student_pred, labels)</span><br><span class="line">        </span><br><span class="line">        distillation_loss = F.kl_div(</span><br><span class="line">            F.log_softmax(teacher_pred / temperature, dim=<span class="number">1</span>),</span><br><span class="line">            F.softmax(student_pred / temperature, dim=<span class="number">1</span>),</span><br><span class="line">            reduction=<span class="string">&#x27;batchmean&#x27;</span>        </span><br><span class="line">        )</span><br><span class="line">        loss = alpha * student_loss + (<span class="number">1</span> - alpha) * distillation_loss</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># print every 100 mini-batches</span></span><br><span class="line">            print(<span class="string">&#x27;[&#123;&#125;, &#123;&#125;] loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">100</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>Lưu ý một vài thay đổi so với model giáo viên:</strong> </p><table><thead><tr><th align="center"><img src="Knowledge-Distillation-voi-PyTorch/knowledge_distillation.png" alt="Knowledge Distillation"></th></tr></thead><tbody><tr><td align="center"><em>Nguồn: <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a></em></td></tr></tbody></table><ul><li>model học sinh sử dụng thêm 1 loại loss nữa, đó là $distillation\ loss$, được tính bằng <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> giữa phân phối của model giáo viên và model học sinh.</li><li>Tham số $alpha$ dùng để đánh trọng số (weight) cho $student\ loss$ và $distillation\ loss$.</li><li>Tham số $temperature$ có tác dụng làm phân phối “soft” hơn. Tham số này càng tăng, thì phân phối càng “soft”, như hình dưới đây:</li></ul><p><img src="Knowledge-Distillation-voi-PyTorch/smoothen.png" alt="Smooth Distribution"></p><h3 id="Tai-sao-khong-train-model-hoc-sinh-tu-dau"><a href="#Tai-sao-khong-train-model-hoc-sinh-tu-dau" class="headerlink" title="Tại sao không train model học sinh từ đầu?"></a>Tại sao không train model học sinh từ đầu?</h3><p>Chúng ta hoàn toàn có thể train model học sinh từ đầu, tuy nhiên sẽ khó đạt được kết quả như kỳ vọng. Kỹ thuật KD giúp model học sinh:</p><ul><li>Generalize tốt hơn, do tập train của model giáo viên thông thường sẽ lớn hơn nhiều so với tập train của model học sinh</li><li>$distillation\ loss$ giảm thiểu ảnh hưởng của việc phân bố tập train của model học sinh khác nhiều so với phân bố thực tế</li></ul><h3 id="Tai-lieu-tham-khao"><a href="#Tai-lieu-tham-khao" class="headerlink" title="Tài liệu tham khảo"></a>Tài liệu tham khảo</h3><p>[1] <a href="https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764">https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764</a><br>[2] <a href="https://keras.io/examples/vision/knowledge_distillation/">https://keras.io/examples/vision/knowledge_distillation/</a><br>[3] <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a><br>[4] <a href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a><br>[5] <a href="https://thenextweb.com/neural/2020/10/26/how-knowledge-distillation-compresses-neural-networks-syndication/">https://thenextweb.com/neural/2020/10/26/how-knowledge-distillation-compresses-neural-networks-syndication/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> knowledge distillation </tag>
            
            <tag> model compression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Trees</title>
      <link href="2021/01/18/decision-trees-dts/"/>
      <url>2021/01/18/decision-trees-dts/</url>
      
        <content type="html"><![CDATA[<div style="text-align: center"> Decision Trees là một trong những thuật toán phổ biến trong Machine Learning, là tiền đề để phát triển các thuật toán phức tạp hơn như Random Forest, XGBoost và LightGBM. Hôm nay mình sẽ cùng nhau tìm hiểu thuật toán này nhé. </div><p><img src="https://hackernoon.com/hn-images/1*6-ctYxKmTS8v0RzwLILfXA.png" alt="Random Forests meme"></p><h3 id="Bai-toan-thuc-te"><a href="#Bai-toan-thuc-te" class="headerlink" title="Bài toán thực tế"></a>Bài toán thực tế</h3><h4 id="Phan-loai-hoa-voi-tap-du-lieu-iris"><a href="#Phan-loai-hoa-voi-tap-du-lieu-iris" class="headerlink" title="Phân loại hoa với tập dữ liệu iris"></a>Phân loại hoa với <a href="https://archive.ics.uci.edu/ml/datasets/iris">tập dữ liệu iris</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># load dữ liệu iris</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># tách features (X) và label (y)</span></span><br><span class="line">X = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># chia dữ liệu thành 2 tập train và test, với tỷ lệ </span></span><br><span class="line"><span class="comment"># tập test là 0.2</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xây dựng model</span></span><br><span class="line">clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># train model</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dự đoán trên tập test</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># đánh giá kết quả dựa trên accuracy score</span></span><br><span class="line">print(<span class="string">&quot;[INFO] accuracy score: &quot;</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><h4 id="Du-doan-gia-nha-voi-tap-du-lieu-boston"><a href="#Du-doan-gia-nha-voi-tap-du-lieu-boston" class="headerlink" title="Dự đoán giá nhà với tập dữ liêu boston"></a>Dự đoán giá nhà với <a href="https://www.kaggle.com/vikrishnan/boston-house-prices">tập dữ liêu boston</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># load dữ liệu boston</span></span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># tách features (X) và label (y)</span></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># chia dữ liệu thành 2 tập train và test, với tỷ lệ </span></span><br><span class="line"><span class="comment"># tập test là 0.2</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xây dựng model</span></span><br><span class="line">clf = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># train model</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dự đoán trên tập test</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># đánh giá kết quả dựa trên r2_score</span></span><br><span class="line">print(<span class="string">&quot;[INFO] r2_score: &quot;</span>, r2_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><h3 id="Y-tuong-thuat-toan"><a href="#Y-tuong-thuat-toan" class="headerlink" title="Ý tưởng thuật toán"></a>Ý tưởng thuật toán</h3><p>Hiểu đơn giản là thuật toán xây dựng một cây quyết định, liên tục phân chia tập dữ liệu về các nhánh. Thuật toán này có thể dùng cho cả bài toán phân loại (classification) và hồi quy (regression).</p><p>Có nhiều phương pháp implement thuật toán Decision Trees, ví dụ như ID3, C4.5, C5.0 và CART. Thư viện sklearn sử dụng phiên bản tối ưu của CART.</p><h4 id="Phan-loai"><a href="#Phan-loai" class="headerlink" title="Phân loại"></a>Phân loại</h4><p>Trở lại bài toán phân loại hoa, sử dụng đoạn code dưới đây để xem cây quyết định trông thế nào nhé.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dot_data = export_graphviz(</span><br><span class="line">            clf,</span><br><span class="line">            out_file=<span class="literal">None</span>,</span><br><span class="line">            feature_names=iris.feature_names,</span><br><span class="line">            class_names=iris.target_names,</span><br><span class="line">            rounded=<span class="literal">True</span>,</span><br><span class="line">            filled=<span class="literal">True</span>,</span><br><span class="line">            special_characters=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line"><span class="comment"># lưu graph ra file graph.png</span></span><br><span class="line">graph.render(<span class="string">&quot;iris-tree&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line"><span class="comment"># hiển thị graph</span></span><br><span class="line">graph</span><br></pre></td></tr></table></figure><p>Và bing-go, cây sẽ như này nè:</p><p><img src="Decision-Trees-DTs/iris-tree.png" alt="iris tree"></p><p><strong>Giải thích cây quyết định:</strong> </p><ul><li><p>Bắt đầu từ node trên cùng (hay node root), với điều kiện $petal\ width \leq 0.8$, dữ liệu nào thỏa mãn điều kiện sẽ đi về nhánh trái, còn lại sẽ đi về nhánh phải. Node màu cam không có nhánh con nào nên được gọi là node lá, và dữ liệu ở node này sẽ được kết luận luôn là thuộc class setosa. Node màu xám (có nhánh con nên được gọi là node phân chia hay splitting node) tiếp tục phân chia tập dữ liệu thuộc node này về 2 node xanh và tím dựa trên điều kiện $petal\ length \leq 4.75$</p></li><li><p>Một số thông tin khác của node bao gồm:</p><ul><li><p>gini: mô tả “độ thuần khiết” (impurity) của node, dao động từ 0 đến 1. Trong đó 0 nghĩa là tất cả dữ liệu thuộc cùng 1 class, nên chúng ta luôn muốn <strong>gini</strong> nhỏ nhất có thể </p><p>  Công thức tính <strong>gini</strong> cho node $i$ như sau: $G_i = 1-\sum_{k=1}^{n}p_{i,k}^2$</p><p>  Trong đó: </p><ul><li><p>$p_{i,k}$ là tỉ lệ số phần tử của class k trên tổng số samples.</p><p>  Ví dụ: ở node màu tím, $G=1-(0/43)^2-(5/43)^2-(38/43)^2\approx0.206$</p></li></ul></li><li><p>samples: số lượng dữ liệu training thuộc node</p></li><li><p>value: số lượng dữ liệu của từng class trong node, ví dụ node tím có 0 setosa, 5 versicolor và 38 virginica </p></li></ul></li></ul><h4 id="Hoi-quy"><a href="#Hoi-quy" class="headerlink" title="Hồi quy"></a>Hồi quy</h4><p>Cây quyết định cho bài toán dự đoán giá nhà như sau:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dot_data = export_graphviz(</span><br><span class="line">            clf,</span><br><span class="line">            out_file=<span class="literal">None</span>,</span><br><span class="line">            feature_names=boston.feature_names,</span><br><span class="line">            rounded=<span class="literal">True</span>,</span><br><span class="line">            filled=<span class="literal">True</span>,</span><br><span class="line">            special_characters=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line"><span class="comment"># lưu graph ra file graph.png</span></span><br><span class="line">graph.render(<span class="string">&quot;boston-tree&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line"><span class="comment"># hiển thị graph</span></span><br><span class="line">graph</span><br></pre></td></tr></table></figure><p><img src="Decision-Trees-DTs/boston-tree.png" alt="boston tree"></p><p>Cây quyết định cho bài toán hồi quy khá giống với phân loại, ngoại trừ:</p><ul><li><p><strong>gini</strong> được thay bằng <strong>MSE</strong></p><p>  Công thức tính <strong>MSE</strong> cho node $i$ như sau: $MSE_i = \sum_{i\in node}(y_i - \bar{y})^2$</p><p>  Trong đó: </p><ul><li>$y_i$ là label của từng phần tử trong node</li><li>$\bar{y} = \frac{1}{tổng\ số \ phần\ tử\ của\ node}\sum_{i\in node}y_i$</li></ul></li><li><p>dự đoán value, thay vì class</p></li></ul><h3 id="Phuong-phap-CART"><a href="#Phuong-phap-CART" class="headerlink" title="Phương pháp CART"></a>Phương pháp CART</h3><ol><li><p>Bước 1: chia tập dữ liệu thành 2 tập con sử dụng feature $k$ và giá trị $t_k$, sao cho hàm cost dưới đây là nhỏ nhất<br> $J(k,t_k)=\frac{m_{trái}}{m}G_{trái}+\frac{m_{phải}}{m}G_{phải}$</p><p> Trong đó:</p><ul><li>$m_{trái}$ và $m_{phải}$ là số lượng dữ liệu của 2 node trái và phải (2 tập con)</li><li>$m$ là số lượng dữ liệu ở node hiện tại</li><li>$G_{trái}$ và $G_{phải}$ là <strong>gini</strong> của 2 node trái và phải</li></ul></li><li><p>Bước 2: tiếp tục phân chia các tập con sử dụng logic tương tự bước trên, cho tới khi thỏa mãn điều kiện dừng, ví dụ như:</p><ul><li>đạt độ sâu tối đa (quy định bởi hyperparameter <strong>max_depth</strong>)</li><li>không thể phân chia sao cho <strong>gini</strong> của các tập con nhỏ hơn tập gốc</li></ul></li></ol><h3 id="Tai-lieu-tham-khao"><a href="#Tai-lieu-tham-khao" class="headerlink" title="Tài liệu tham khảo"></a>Tài liệu tham khảo</h3><p>[1] Aurlien Gron. 2020. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd. ed.). O’Reilly Media, Inc.<br>[2] Brownlee, J. (2016) Machine Learning Mastery with Python. Machine Learning Mastery, EBook.<br>[3] <a href="https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052">https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> decision trees </tag>
            
            <tag> random forest </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
